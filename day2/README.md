# Day 2

Read Chapter 2 of PMPP

## Notes

- Data Parallelism:
  - Reorganize the compution around the data
    - Like a blur operation only needs the data about its nearby pixels
  - Main source of scalibity
- Task Parallelism
  - Decomposing the tasks according to the program
- **CUDA C code start:**
  - Host - CPU
  - Devices (GPUs)
  - All  "normal" C code is CUDA code with a single host and no devices
  - Compiler: **NVCC (NVIDIA C Compiler)**
  - Data parallel functions are called: **kernels**
  - When a kernel function (parallel device code) is
    called, or launched, it is executed by a large number of threads on a device. All the
    threads that are generated by a kernel launch are collectively called a grid. These
    threads are the primary vehicle of parallel execution in a CUDA platform
- **Threads**
  - A thread is a simplified view of how a processor executes a sequential pro
    gram in modern computers. A thread consists of the code of the program,
    the particular point in the code that is being executed, and the values of its
    variables and data structures. The execution of a thread is sequential as far
    as a user is concerned. One can use a source-level debugger to monitor the
    progress of a thread by executing one statement at a time, looking at the state
    ment that will be executed next and checking the values of the variables and
    data structures as the execution progresses.
  - Threads have been used in programming for many years. If a programmer
    wants to start parallel execution in an application, he/she creates and man
    ages multiple threads using thread libraries or special languages. In CUDA,
    the execution of each thread is sequential as well. A CUDA program initiates
    parallel execution by launching kernel functions, which causes the underlying
    run-time mechanisms to create many threads that process different parts of the
    data in parallel
- **Cuda C** continued
  - In order to execute a kernel on a device, the programmer needs to allocate global memory on the device and
    transfer pertinent data from the host memory to the allocated device memory.

    ```cpp
    // The cudaMalloc function can be called from the host code to allocate a 
    // piece of device global memory for an object. The reader should notice the striking 
    // similarity between cudaMalloc and the standard C run-time library malloc function. 
    ```
  - `cudaMalloc()`
    • Allocates object in the device global memory
    • Two parameters

    - Address of a pointer to the allocated object
    - Size of allocated object in terms of bytes
  - `cudaFree()`
    • Frees object from device global memory

    - Pointer to freed object
